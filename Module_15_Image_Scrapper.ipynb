{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZG-YUTZBsII7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to this given URL and solve the following questions\n",
        "URL: https://www.youtube.com/@PW-Foundation/videos"
      ],
      "metadata": {
        "id": "2bnktpEksQG4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Write a python program to extract the video URL of the first five videos."
      ],
      "metadata": {
        "id": "4AM8QWwBsQ5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the webpage containing the list of videos\n",
        "url = 'https://example.com/videos'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the webpage using Beautiful Soup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all the <a> tags containing video URLs\n",
        "    video_links = soup.find_all('a', href=True)\n",
        "\n",
        "    # Extract URLs of the videos for the first five videos\n",
        "    count = 0\n",
        "    for link in video_links:\n",
        "        video_url = link['href']  # Extract the value of the 'href' attribute\n",
        "        if video_url.startswith('https://www.youtube.com/watch?v='):  # Assuming YouTube video URLs\n",
        "            count += 1\n",
        "            print(f\"Video {count}: {video_url}\")  # Print the URL of the video\n",
        "            if count == 5:  # Exit the loop after finding the first five video URLs\n",
        "                break\n",
        "else:\n",
        "    print('Failed to retrieve webpage. Status code:', response.status_code)"
      ],
      "metadata": {
        "id": "VLJVKWDLEar8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. Write a python program to extract the URL of the video thumbnails of the first five videos."
      ],
      "metadata": {
        "id": "3uLkEpKJDjZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the webpage containing the list of videos\n",
        "url = 'https://example.com/videos'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the webpage using Beautiful Soup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all the <img> tags containing video thumbnails\n",
        "    thumbnails = soup.find_all('img')\n",
        "\n",
        "    # Extract URLs of the video thumbnails for the first five videos\n",
        "    for i, thumbnail in enumerate(thumbnails[:5], 1):\n",
        "        thumbnail_url = thumbnail['src']  # Extract the value of the 'src' attribute\n",
        "        print(f\"Thumbnail URL for Video {i}: {thumbnail_url}\")  # Print the URL of the video thumbnail\n",
        "else:\n",
        "    print('Failed to retrieve webpage. Status code:', response.status_code)"
      ],
      "metadata": {
        "id": "iDBFOTZ0EOvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Write a python program to extract the title of the first five videos."
      ],
      "metadata": {
        "id": "9hU20rJRDmy3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the webpage containing the list of videos\n",
        "url = 'https://example.com/videos'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the webpage using Beautiful Soup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all the elements containing video titles (assuming they are wrapped in <h2> tags)\n",
        "    video_titles = soup.find_all('h2')\n",
        "\n",
        "    # Extract titles of the first five videos\n",
        "    for i, title in enumerate(video_titles[:5], 1):\n",
        "        print(f\"Video {i}: {title.text.strip()}\")  # Print the title of the video\n",
        "else:\n",
        "    print('Failed to retrieve webpage. Status code:', response.status_code)"
      ],
      "metadata": {
        "id": "dq8iWt3aDyP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. Write a python program to extract the number of views of the first five videos."
      ],
      "metadata": {
        "id": "H1m-b1iyD2H4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the webpage containing the list of videos\n",
        "url = 'https://example.com/videos'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the webpage using Beautiful Soup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all the elements containing video views information (assuming they are wrapped in <span> tags with a specific class)\n",
        "    video_views = soup.find_all('span', class_='views')\n",
        "\n",
        "    # Extract views of the first five videos\n",
        "    for i, views in enumerate(video_views[:5], 1):\n",
        "        print(f\"Video {i}: {views.text.strip()}\")  # Print the number of views of the video\n",
        "else:\n",
        "    print('Failed to retrieve webpage. Status code:', response.status_code)"
      ],
      "metadata": {
        "id": "R5yb6SklD3TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. Write a python program to extract the time of posting of video for the first five videos."
      ],
      "metadata": {
        "id": "fub8PjbaD-04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL of the webpage containing the list of videos\n",
        "url = 'https://example.com/videos'\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Parse the HTML content of the webpage using Beautiful Soup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all the elements containing time of posting information (assuming they are wrapped in <span> tags with a specific class)\n",
        "    post_times = soup.find_all('span', class_='post-time')\n",
        "\n",
        "    # Extract time of posting for the first five videos\n",
        "    for i, post_time in enumerate(post_times[:5], 1):\n",
        "        print(f\"Video {i}: {post_time.text.strip()}\")  # Print the time of posting of the video\n",
        "else:\n",
        "    print('Failed to retrieve webpage. Status code:', response.status_code)"
      ],
      "metadata": {
        "id": "2EQEi3u3EH0K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}